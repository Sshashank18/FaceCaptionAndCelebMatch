{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cea9df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:17.195215Z",
     "iopub.status.busy": "2025-05-08T19:39:17.194953Z",
     "iopub.status.idle": "2025-05-08T19:39:24.175516Z",
     "shell.execute_reply": "2025-05-08T19:39:24.174561Z"
    },
    "papermill": {
     "duration": 6.989064,
     "end_time": "2025-05-08T19:39:24.177542",
     "exception": false,
     "start_time": "2025-05-08T19:39:17.188478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c792a8cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:24.189806Z",
     "iopub.status.busy": "2025-05-08T19:39:24.189155Z",
     "iopub.status.idle": "2025-05-08T19:39:24.246893Z",
     "shell.execute_reply": "2025-05-08T19:39:24.245991Z"
    },
    "papermill": {
     "duration": 0.066373,
     "end_time": "2025-05-08T19:39:24.248869",
     "exception": false,
     "start_time": "2025-05-08T19:39:24.182496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Load Raw Data for Vocabulary\n",
    "csv_file = '/kaggle/input/faceattdb/final_version.csv'\n",
    "image_folder = '/kaggle/input/faceattdb/images/'\n",
    "\n",
    "data = pd.read_csv(csv_file)\n",
    "captions = data['description'].tolist()  # Extract captions from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edcb830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:24.259433Z",
     "iopub.status.busy": "2025-05-08T19:39:24.259156Z",
     "iopub.status.idle": "2025-05-08T19:39:24.268240Z",
     "shell.execute_reply": "2025-05-08T19:39:24.267501Z"
    },
    "papermill": {
     "duration": 0.016214,
     "end_time": "2025-05-08T19:39:24.269817",
     "exception": false,
     "start_time": "2025-05-08T19:39:24.253603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        # Tokenize captions and build vocabulary\n",
    "        self.vocab = self.build_vocab(self.data['description'].tolist())\n",
    "        self.tokenized_captions = [self.caption_to_seq(caption) for caption in self.data['description']]\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        tokens = []\n",
    "        for caption in captions:\n",
    "            tokens.extend(word_tokenize(caption.lower()))\n",
    "        counter = Counter(tokens) #Count how many times a word appeaared in token list Ex: {'a': 10, 'man': 5, 'with': 4, 'dog': 3, ...}\n",
    "        vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.items())} #Each word getting unique index st. from 2 (.items() will help in getting unique words)\n",
    "        vocab[\"<PAD>\"] = 0\n",
    "        vocab[\"<SOS>\"] = 1\n",
    "        vocab[\"<EOS>\"] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def caption_to_seq(self, caption):\n",
    "        tokens = word_tokenize(caption.lower())\n",
    "        seq = [self.vocab[\"<SOS>\"]] + [self.vocab.get(token, self.vocab[\"<PAD>\"]) for token in tokens] + [self.vocab[\"<EOS>\"]]\n",
    "        return seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.data.iloc[idx,0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        caption = self.tokenized_captions[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5f5568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:24.279826Z",
     "iopub.status.busy": "2025-05-08T19:39:24.279556Z",
     "iopub.status.idle": "2025-05-08T19:39:24.283438Z",
     "shell.execute_reply": "2025-05-08T19:39:24.282744Z"
    },
    "papermill": {
     "duration": 0.010766,
     "end_time": "2025-05-08T19:39:24.284971",
     "exception": false,
     "start_time": "2025-05-08T19:39:24.274205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393dac1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:24.294824Z",
     "iopub.status.busy": "2025-05-08T19:39:24.294544Z",
     "iopub.status.idle": "2025-05-08T19:39:24.352388Z",
     "shell.execute_reply": "2025-05-08T19:39:24.351541Z"
    },
    "papermill": {
     "duration": 0.06468,
     "end_time": "2025-05-08T19:39:24.354125",
     "exception": false,
     "start_time": "2025-05-08T19:39:24.289445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30\n",
    "MAX_SEQ_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf6bce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:24.364253Z",
     "iopub.status.busy": "2025-05-08T19:39:24.363979Z",
     "iopub.status.idle": "2025-05-08T19:39:25.230225Z",
     "shell.execute_reply": "2025-05-08T19:39:25.229247Z"
    },
    "papermill": {
     "duration": 0.873616,
     "end_time": "2025-05-08T19:39:25.232370",
     "exception": false,
     "start_time": "2025-05-08T19:39:24.358754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset =ImageCaptionDataset(csv_file=\"/kaggle/input/faceattdb/final_version.csv\", image_folder=\"/kaggle/input/faceattdb/images\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: zip(*x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c38f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:25.243374Z",
     "iopub.status.busy": "2025-05-08T19:39:25.242664Z",
     "iopub.status.idle": "2025-05-08T19:39:25.248250Z",
     "shell.execute_reply": "2025-05-08T19:39:25.247468Z"
    },
    "papermill": {
     "duration": 0.012629,
     "end_time": "2025-05-08T19:39:25.249839",
     "exception": false,
     "start_time": "2025-05-08T19:39:25.237210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)  # Access the in_features from the ResNet FC layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn(images).squeeze(-1).squeeze(-1)  # Flatten AdaptiveAvgPool2d output\n",
    "        features = self.fc(features)  # Map to the embedding size\n",
    "        return self.relu(self.dropout(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd29a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:25.260194Z",
     "iopub.status.busy": "2025-05-08T19:39:25.259953Z",
     "iopub.status.idle": "2025-05-08T19:39:25.265124Z",
     "shell.execute_reply": "2025-05-08T19:39:25.264326Z"
    },
    "papermill": {
     "duration": 0.012476,
     "end_time": "2025-05-08T19:39:25.266738",
     "exception": false,
     "start_time": "2025-05-08T19:39:25.254262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])  # Skip <EOS>\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4018cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:25.277237Z",
     "iopub.status.busy": "2025-05-08T19:39:25.276454Z",
     "iopub.status.idle": "2025-05-08T19:39:25.281111Z",
     "shell.execute_reply": "2025-05-08T19:39:25.280356Z"
    },
    "papermill": {
     "duration": 0.011544,
     "end_time": "2025-05-08T19:39:25.282743",
     "exception": false,
     "start_time": "2025-05-08T19:39:25.271199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#combined model\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)  # Extract image features\n",
    "        outputs = self.decoder(features, captions)  # Generate captions\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae913f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:25.293029Z",
     "iopub.status.busy": "2025-05-08T19:39:25.292560Z",
     "iopub.status.idle": "2025-05-08T19:39:25.298170Z",
     "shell.execute_reply": "2025-05-08T19:39:25.297394Z"
    },
    "papermill": {
     "duration": 0.012772,
     "end_time": "2025-05-08T19:39:25.299969",
     "exception": false,
     "start_time": "2025-05-08T19:39:25.287197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de494069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:25.310333Z",
     "iopub.status.busy": "2025-05-08T19:39:25.310093Z",
     "iopub.status.idle": "2025-05-08T19:39:26.786665Z",
     "shell.execute_reply": "2025-05-08T19:39:26.785993Z"
    },
    "papermill": {
     "duration": 1.484107,
     "end_time": "2025-05-08T19:39:26.788625",
     "exception": false,
     "start_time": "2025-05-08T19:39:25.304518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 213MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "vocab_size = len(dataset.vocab)\n",
    "encoder = CNNEncoder(embed_size=EMBED_SIZE).to(device)\n",
    "decoder = LSTMDecoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS).to(device)\n",
    "model = ImageCaptioningModel(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187c1e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:26.800602Z",
     "iopub.status.busy": "2025-05-08T19:39:26.799964Z",
     "iopub.status.idle": "2025-05-08T19:39:26.804837Z",
     "shell.execute_reply": "2025-05-08T19:39:26.804036Z"
    },
    "papermill": {
     "duration": 0.012333,
     "end_time": "2025-05-08T19:39:26.806354",
     "exception": false,
     "start_time": "2025-05-08T19:39:26.794021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa3e0aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:39:26.817498Z",
     "iopub.status.busy": "2025-05-08T19:39:26.817067Z",
     "iopub.status.idle": "2025-05-08T19:44:45.563245Z",
     "shell.execute_reply": "2025-05-08T19:44:45.561850Z"
    },
    "papermill": {
     "duration": 318.765813,
     "end_time": "2025-05-08T19:44:45.577297",
     "exception": false,
     "start_time": "2025-05-08T19:39:26.811484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 4.3802\n",
      "Epoch [2/30], Loss: 3.9387\n",
      "Epoch [3/30], Loss: 3.8662\n",
      "Epoch [4/30], Loss: 3.8060\n",
      "Epoch [5/30], Loss: 3.5882\n",
      "Epoch [6/30], Loss: 3.2559\n",
      "Epoch [7/30], Loss: 2.8865\n",
      "Epoch [8/30], Loss: 2.5667\n",
      "Epoch [9/30], Loss: 2.3596\n",
      "Epoch [10/30], Loss: 2.2294\n",
      "Epoch [11/30], Loss: 2.1271\n",
      "Epoch [12/30], Loss: 2.0412\n",
      "Epoch [13/30], Loss: 1.9688\n",
      "Epoch [14/30], Loss: 1.9033\n",
      "Epoch [15/30], Loss: 1.8405\n",
      "Epoch [16/30], Loss: 1.7763\n",
      "Epoch [17/30], Loss: 1.7263\n",
      "Epoch [18/30], Loss: 1.6736\n",
      "Epoch [19/30], Loss: 1.6338\n",
      "Epoch [20/30], Loss: 1.5610\n",
      "Epoch [21/30], Loss: 1.5134\n",
      "Epoch [22/30], Loss: 1.4683\n",
      "Epoch [23/30], Loss: 1.4146\n",
      "Epoch [24/30], Loss: 1.3665\n",
      "Epoch [25/30], Loss: 1.3185\n",
      "Epoch [26/30], Loss: 1.2719\n",
      "Epoch [27/30], Loss: 1.2225\n",
      "Epoch [28/30], Loss: 1.1826\n",
      "Epoch [29/30], Loss: 1.1175\n",
      "Epoch [30/30], Loss: 1.0708\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, captions in dataloader:\n",
    "        images = torch.stack(images).to(device)\n",
    "        captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=dataset.vocab[\"<PAD>\"]).to(device)   #Padding for same length in tensor\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, captions)\n",
    "        targets = captions[:, 1:]  # Skip <SOS>\n",
    "        outputs = outputs[:, :targets.shape[1], :]  # Align sequence lengths\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {epoch_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f73f0a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:45.598706Z",
     "iopub.status.busy": "2025-05-08T19:44:45.598236Z",
     "iopub.status.idle": "2025-05-08T19:44:45.719582Z",
     "shell.execute_reply": "2025-05-08T19:44:45.718697Z"
    },
    "papermill": {
     "duration": 0.134938,
     "end_time": "2025-05-08T19:44:45.721510",
     "exception": false,
     "start_time": "2025-05-08T19:44:45.586572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: this an attractive with hair is straight and , young male oval\n"
     ]
    }
   ],
   "source": [
    "def generate_caption(image, model, vocab, max_length=20):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Extract features using the encoder part of the model\n",
    "        features = model.encoder(image.unsqueeze(0).to(device))\n",
    "        \n",
    "        # Initialize caption generation with the <SOS> token\n",
    "        caption = [vocab[\"<SOS>\"]]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Prepare input: current caption sequence\n",
    "            inputs = torch.tensor(caption).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate the next word using the decoder\n",
    "            outputs = model.decoder(features, inputs)\n",
    "            predicted = outputs.argmax(2)[:, -1].item()  # Get the index of the most probable word\n",
    "            \n",
    "            # Stop if the <EOS> token is generated\n",
    "            if predicted == vocab[\"<EOS>\"]:\n",
    "                break\n",
    "            \n",
    "            # Append the predicted word to the caption sequence\n",
    "            caption.append(predicted)\n",
    "        \n",
    "        # Convert token indices back to words\n",
    "        caption_tokens = [k for k, v in vocab.items() if v in caption and v not in {vocab[\"<SOS>\"], vocab[\"<EOS>\"]}]\n",
    "        return \" \".join(caption_tokens)\n",
    "\n",
    "# Test with an image\n",
    "test_image = transform(Image.open(\"/kaggle/input/face-img/picture.jpeg\").convert('RGB'))\n",
    "\n",
    "# Generate a caption\n",
    "generated_caption = generate_caption(test_image, model, dataset.vocab)\n",
    "print(\"Generated Caption:\", generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48fdc6bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:45.736147Z",
     "iopub.status.busy": "2025-05-08T19:44:45.735871Z",
     "iopub.status.idle": "2025-05-08T19:44:46.099557Z",
     "shell.execute_reply": "2025-05-08T19:44:46.098582Z"
    },
    "papermill": {
     "duration": 0.373862,
     "end_time": "2025-05-08T19:44:46.102171",
     "exception": false,
     "start_time": "2025-05-08T19:44:45.728309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"ImageCaptioning.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62656656",
   "metadata": {
    "papermill": {
     "duration": 0.008685,
     "end_time": "2025-05-08T19:44:46.123245",
     "exception": false,
     "start_time": "2025-05-08T19:44:46.114560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Now Matching with the closest celebrity Bollywood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81430e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:46.136969Z",
     "iopub.status.busy": "2025-05-08T19:44:46.136662Z",
     "iopub.status.idle": "2025-05-08T19:44:46.140627Z",
     "shell.execute_reply": "2025-05-08T19:44:46.139879Z"
    },
    "papermill": {
     "duration": 0.013164,
     "end_time": "2025-05-08T19:44:46.142727",
     "exception": false,
     "start_time": "2025-05-08T19:44:46.129563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81805fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:46.163290Z",
     "iopub.status.busy": "2025-05-08T19:44:46.163047Z",
     "iopub.status.idle": "2025-05-08T19:44:46.167732Z",
     "shell.execute_reply": "2025-05-08T19:44:46.166913Z"
    },
    "papermill": {
     "duration": 0.014766,
     "end_time": "2025-05-08T19:44:46.169393",
     "exception": false,
     "start_time": "2025-05-08T19:44:46.154627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Feature Extraction Function\n",
    "def extract_features(image_path, model, transform):\n",
    "\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(image_tensor).squeeze(0)\n",
    "        features = features / features.norm(p=2)  # Normalize the feature vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "117d3f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:46.183499Z",
     "iopub.status.busy": "2025-05-08T19:44:46.182911Z",
     "iopub.status.idle": "2025-05-08T19:44:46.191856Z",
     "shell.execute_reply": "2025-05-08T19:44:46.191083Z"
    },
    "papermill": {
     "duration": 0.017815,
     "end_time": "2025-05-08T19:44:46.193543",
     "exception": false,
     "start_time": "2025-05-08T19:44:46.175728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Build Celebrity Database\n",
    "def build_celebrity_database(dataset_path, model, transform):\n",
    "\n",
    "    celebrity_features = {}\n",
    "    # Traverse through all parent folders\n",
    "    for parent_folder in os.listdir(dataset_path):\n",
    "        parent_path = os.path.join(dataset_path, parent_folder)\n",
    "        if not os.path.isdir(parent_path):\n",
    "            continue\n",
    "\n",
    "        # Traverse through celebrity folders in each parent folder\n",
    "        for celebrity_name in os.listdir(parent_path):\n",
    "            celebrity_folder = os.path.join(parent_path, celebrity_name)\n",
    "            if not os.path.isdir(celebrity_folder):\n",
    "                continue\n",
    "\n",
    "            # Extract features for all images of this celebrity\n",
    "            all_features = []\n",
    "            for image_name in os.listdir(celebrity_folder):\n",
    "                image_path = os.path.join(celebrity_folder, image_name)\n",
    "                try:\n",
    "                    features = extract_features(image_path, model, transform)\n",
    "                    all_features.append(features)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping {image_path}: {e}\")\n",
    "\n",
    "            # Compute the average feature vector for the celebrity\n",
    "            if all_features:\n",
    "                average_features = torch.stack(all_features).mean(dim=0)\n",
    "                celebrity_features[celebrity_name] = average_features\n",
    "\n",
    "    return celebrity_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ad1231b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:46.207234Z",
     "iopub.status.busy": "2025-05-08T19:44:46.206927Z",
     "iopub.status.idle": "2025-05-08T19:44:46.212280Z",
     "shell.execute_reply": "2025-05-08T19:44:46.211549Z"
    },
    "papermill": {
     "duration": 0.014165,
     "end_time": "2025-05-08T19:44:46.214012",
     "exception": false,
     "start_time": "2025-05-08T19:44:46.199847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Match Against Celebrity Database\n",
    "def match_with_celebrity(test_features, celebrity_features):\n",
    "\n",
    "    best_match = None\n",
    "    highest_similarity = -1\n",
    "\n",
    "    for name, features in celebrity_features.items():\n",
    "        similarity = torch.dot(test_features, features).item()  # Cosine similarity\n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            best_match = name\n",
    "\n",
    "    return best_match, highest_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fcc85bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:46.227756Z",
     "iopub.status.busy": "2025-05-08T19:44:46.227424Z",
     "iopub.status.idle": "2025-05-08T19:44:46.231348Z",
     "shell.execute_reply": "2025-05-08T19:44:46.230616Z"
    },
    "papermill": {
     "duration": 0.012592,
     "end_time": "2025-05-08T19:44:46.232886",
     "exception": false,
     "start_time": "2025-05-08T19:44:46.220294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input/bollywood-celeb-localized-face-dataset/Bollywood_celeb_face_localized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03b4ad91",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:46.246636Z",
     "iopub.status.busy": "2025-05-08T19:44:46.246109Z",
     "iopub.status.idle": "2025-05-08T19:44:47.818405Z",
     "shell.execute_reply": "2025-05-08T19:44:47.817498Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.581145,
     "end_time": "2025-05-08T19:44:47.820204",
     "exception": false,
     "start_time": "2025-05-08T19:44:46.239059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2381253069.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/kaggle/input/face-image-captioning/pytorch/default/1/ImageCaptioning .pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageCaptioningModel(\n",
       "  (encoder): CNNEncoder(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): LSTMDecoder(\n",
       "    (embed): Embedding(537, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=3, batch_first=True)\n",
       "    (fc): Linear(in_features=512, out_features=537, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Initialize and load the trained model\n",
    "vocab_size = len(dataset.vocab)\n",
    "encoder = CNNEncoder(EMBED_SIZE).to(device)\n",
    "decoder = LSTMDecoder(EMBED_SIZE, HIDDEN_SIZE, vocab_size, NUM_LAYERS).to(device)\n",
    "model = ImageCaptioningModel(encoder, decoder).to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/face-image-captioning/pytorch/default/1/ImageCaptioning .pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2750d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:44:47.835431Z",
     "iopub.status.busy": "2025-05-08T19:44:47.834766Z",
     "iopub.status.idle": "2025-05-08T19:46:32.119542Z",
     "shell.execute_reply": "2025-05-08T19:46:32.118574Z"
    },
    "papermill": {
     "duration": 104.294586,
     "end_time": "2025-05-08T19:46:32.121807",
     "exception": false,
     "start_time": "2025-05-08T19:44:47.827221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building celebrity database...\n"
     ]
    }
   ],
   "source": [
    "# Build the celebrity database\n",
    "print(\"Building celebrity database...\")\n",
    "celebrity_features = build_celebrity_database(dataset_path, model, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57562b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:46:32.137324Z",
     "iopub.status.busy": "2025-05-08T19:46:32.137046Z",
     "iopub.status.idle": "2025-05-08T19:46:32.168175Z",
     "shell.execute_reply": "2025-05-08T19:46:32.167329Z"
    },
    "papermill": {
     "duration": 0.040822,
     "end_time": "2025-05-08T19:46:32.169982",
     "exception": false,
     "start_time": "2025-05-08T19:46:32.129160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching test image...\n",
      "Matched Celebrity: Preity_Zinta\n",
      "Similarity Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    " # Test image path\n",
    "test_image_path = \"/kaggle/input/face-img/picture.jpeg\"\n",
    "print(\"Matching test image...\")\n",
    "test_features = extract_features(test_image_path, model, transform)\n",
    "match_name, similarity = match_with_celebrity(test_features, celebrity_features)\n",
    "\n",
    "print(f\"Matched Celebrity: {match_name}\")\n",
    "print(f\"Similarity Score: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5db2bfb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:46:32.188633Z",
     "iopub.status.busy": "2025-05-08T19:46:32.188106Z",
     "iopub.status.idle": "2025-05-08T19:46:32.192573Z",
     "shell.execute_reply": "2025-05-08T19:46:32.191890Z"
    },
    "papermill": {
     "duration": 0.014542,
     "end_time": "2025-05-08T19:46:32.194128",
     "exception": false,
     "start_time": "2025-05-08T19:46:32.179586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_celebrity_features(celebrity_features, save_path):\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(celebrity_features, f)\n",
    "    print(f\"Celebrity features saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18d9b215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:46:32.208925Z",
     "iopub.status.busy": "2025-05-08T19:46:32.208281Z",
     "iopub.status.idle": "2025-05-08T19:46:32.213409Z",
     "shell.execute_reply": "2025-05-08T19:46:32.212786Z"
    },
    "papermill": {
     "duration": 0.014106,
     "end_time": "2025-05-08T19:46:32.214952",
     "exception": false,
     "start_time": "2025-05-08T19:46:32.200846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_celebrity_features(load_path):\n",
    "    with open(load_path, \"rb\") as f:\n",
    "        celebrity_features = pickle.load(f)\n",
    "    print(f\"Celebrity features loaded from {load_path}\")\n",
    "    return celebrity_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4596560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T19:46:32.229587Z",
     "iopub.status.busy": "2025-05-08T19:46:32.228963Z",
     "iopub.status.idle": "2025-05-08T19:46:32.245313Z",
     "shell.execute_reply": "2025-05-08T19:46:32.244472Z"
    },
    "papermill": {
     "duration": 0.02533,
     "end_time": "2025-05-08T19:46:32.246816",
     "exception": false,
     "start_time": "2025-05-08T19:46:32.221486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celebrity features saved to /kaggle/working/celebrity_features.pkl\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/kaggle/working/celebrity_features.pkl\"\n",
    "save_celebrity_features(celebrity_features, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8c9f1",
   "metadata": {
    "papermill": {
     "duration": 0.006488,
     "end_time": "2025-05-08T19:46:32.260082",
     "exception": false,
     "start_time": "2025-05-08T19:46:32.253594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 979040,
     "sourceId": 1654116,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5207525,
     "sourceId": 9805467,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6354384,
     "sourceId": 10270365,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 201725,
     "modelInstanceId": 179455,
     "sourceId": 210498,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 439.808766,
   "end_time": "2025-05-08T19:46:34.569918",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-08T19:39:14.761152",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
